{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb55a593",
   "metadata": {},
   "source": [
    "# Phase 6: Dashboard Data Merge\n",
    "\n",
    "## Aadhaar Pulse - Final Backend-Ready Dataset\n",
    "\n",
    "**Objective:** Merge all Phase 3-5 outputs into a single, unified dataset for backend APIs and frontend dashboards.\n",
    "\n",
    "### Input Files:\n",
    "| File | Source Phase | Fields |\n",
    "|------|--------------|--------|\n",
    "| `migration_intensity.csv` | Phase 3 | `migration_score`, `migration_category` |\n",
    "| `peri_urban_labels.csv` | Phase 4 | `peri_urban_label` |\n",
    "| `digital_exclusion_risk.csv` | Phase 5 | `digital_exclusion_score`, `risk_level` |\n",
    "\n",
    "### Output Files:\n",
    "| File | Format | Key Convention |\n",
    "|------|--------|----------------|\n",
    "| `aadhaar_pulse_dashboard.csv` | CSV | `snake_case` |\n",
    "| `aadhaar_pulse_dashboard.json` | JSON | `camelCase` |\n",
    "\n",
    "### Rules:\n",
    "- ‚ùå NO new logic or recomputation\n",
    "- ‚ùå NO value modifications\n",
    "- ‚úÖ Merge on `district_id` + `month`\n",
    "- ‚úÖ Preserve all original values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc61dd",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Configure Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfd0e456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported and paths configured\n",
      "\n",
      "üìÅ Input files:\n",
      "   Migration:         d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\final\\migration_intensity.csv\n",
      "   Peri-urban:        d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\final\\peri_urban_labels.csv\n",
      "   Digital exclusion: d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\final\\digital_exclusion_risk.csv\n",
      "\n",
      "üìÅ Output directory: d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\api_ready\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 6: Dashboard Data Merge\n",
    "-----------------------------\n",
    "This notebook merges all intelligence outputs into a single backend-ready dataset.\n",
    "\n",
    "NO new logic is applied - only merging and formatting.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# PATH CONFIGURATION\n",
    "# =============================================================================\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "\n",
    "# Input paths (from previous phases)\n",
    "MIGRATION_PATH = PROJECT_ROOT / 'data' / 'final' / 'migration_intensity.csv'\n",
    "PERI_URBAN_PATH = PROJECT_ROOT / 'data' / 'final' / 'peri_urban_labels.csv'\n",
    "DIGITAL_EXCLUSION_PATH = PROJECT_ROOT / 'data' / 'final' / 'digital_exclusion_risk.csv'\n",
    "\n",
    "# Output paths\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'data' / 'api_ready'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CSV_OUTPUT_PATH = OUTPUT_DIR / 'aadhaar_pulse_dashboard.csv'\n",
    "JSON_OUTPUT_PATH = OUTPUT_DIR / 'aadhaar_pulse_dashboard.json'\n",
    "\n",
    "print(\"‚úÖ Libraries imported and paths configured\")\n",
    "print(f\"\\nüìÅ Input files:\")\n",
    "print(f\"   Migration:         {MIGRATION_PATH}\")\n",
    "print(f\"   Peri-urban:        {PERI_URBAN_PATH}\")\n",
    "print(f\"   Digital exclusion: {DIGITAL_EXCLUSION_PATH}\")\n",
    "print(f\"\\nüìÅ Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71817509",
   "metadata": {},
   "source": [
    "## 2. Load Input Datasets\n",
    "\n",
    "Load all three phase outputs and verify their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f89d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Migration data loaded: 240 rows\n",
      "   Columns: ['district_id', 'month', 'migration_score', 'migration_category']\n",
      "\n",
      "‚úÖ Peri-urban data loaded: 240 rows\n",
      "   Columns: ['district_id', 'month', 'peri_urban_label', 'growth_confidence']\n",
      "\n",
      "‚úÖ Digital exclusion data loaded: 240 rows\n",
      "   Columns: ['district_id', 'month', 'digital_exclusion_score', 'risk_level']\n",
      "\n",
      "‚úÖ Row count consistency verified: 240 rows each\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load Input Datasets\n",
    "-------------------\n",
    "Load outputs from Phases 3, 4, and 5.\n",
    "Verify row counts match before merging.\n",
    "\"\"\"\n",
    "\n",
    "# Load Phase 3: Migration Intensity\n",
    "# Columns: district_id, month, migration_score, migration_category\n",
    "migration_df = pd.read_csv(MIGRATION_PATH)\n",
    "print(f\"‚úÖ Migration data loaded: {len(migration_df)} rows\")\n",
    "print(f\"   Columns: {list(migration_df.columns)}\")\n",
    "\n",
    "# Load Phase 4: Peri-Urban Labels\n",
    "# Columns: district_id, month, peri_urban_label, growth_confidence\n",
    "peri_urban_df = pd.read_csv(PERI_URBAN_PATH)\n",
    "print(f\"\\n‚úÖ Peri-urban data loaded: {len(peri_urban_df)} rows\")\n",
    "print(f\"   Columns: {list(peri_urban_df.columns)}\")\n",
    "\n",
    "# Load Phase 5: Digital Exclusion Risk\n",
    "# Columns: district_id, month, digital_exclusion_score, risk_level\n",
    "digital_df = pd.read_csv(DIGITAL_EXCLUSION_PATH)\n",
    "print(f\"\\n‚úÖ Digital exclusion data loaded: {len(digital_df)} rows\")\n",
    "print(f\"   Columns: {list(digital_df.columns)}\")\n",
    "\n",
    "# Verify row count consistency\n",
    "assert len(migration_df) == len(peri_urban_df) == len(digital_df), \\\n",
    "    \"‚ùå Row count mismatch across input files!\"\n",
    "print(f\"\\n‚úÖ Row count consistency verified: {len(migration_df)} rows each\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a0387f",
   "metadata": {},
   "source": [
    "## 3. Merge Datasets\n",
    "\n",
    "Merge all three datasets on `district_id` + `month` keys.\n",
    "Only select required columns for the final schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e417de34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Base migration data - 240 rows\n",
      "Step 2: After peri-urban merge - 240 rows\n",
      "Step 3: After digital exclusion merge - 240 rows\n",
      "\n",
      "‚úÖ Merge complete: 240 rows preserved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Merge Datasets\n",
    "--------------\n",
    "Step 1: Start with migration data (has district_id, month, migration_score, migration_category)\n",
    "Step 2: Merge peri-urban data (add peri_urban_label)\n",
    "Step 3: Merge digital exclusion data (add digital_exclusion_score, risk_level)\n",
    "\n",
    "Merge key: district_id + month\n",
    "Join type: inner (all datasets should have identical keys)\n",
    "\"\"\"\n",
    "\n",
    "# Step 1: Start with migration data\n",
    "# Keep: district_id, month, migration_score, migration_category\n",
    "dashboard_df = migration_df[['district_id', 'month', 'migration_score', 'migration_category']].copy()\n",
    "print(f\"Step 1: Base migration data - {len(dashboard_df)} rows\")\n",
    "\n",
    "# Step 2: Merge peri-urban labels\n",
    "# Keep only: peri_urban_label (drop growth_confidence - not in final schema)\n",
    "peri_urban_subset = peri_urban_df[['district_id', 'month', 'peri_urban_label']]\n",
    "dashboard_df = dashboard_df.merge(\n",
    "    peri_urban_subset,\n",
    "    on=['district_id', 'month'],\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Step 2: After peri-urban merge - {len(dashboard_df)} rows\")\n",
    "\n",
    "# Step 3: Merge digital exclusion data\n",
    "# Keep: digital_exclusion_score, risk_level\n",
    "digital_subset = digital_df[['district_id', 'month', 'digital_exclusion_score', 'risk_level']]\n",
    "dashboard_df = dashboard_df.merge(\n",
    "    digital_subset,\n",
    "    on=['district_id', 'month'],\n",
    "    how='inner'\n",
    ")\n",
    "print(f\"Step 3: After digital exclusion merge - {len(dashboard_df)} rows\")\n",
    "\n",
    "# Verify no rows lost during merge\n",
    "assert len(dashboard_df) == len(migration_df), \\\n",
    "    f\"‚ùå Row loss during merge! Expected {len(migration_df)}, got {len(dashboard_df)}\"\n",
    "print(f\"\\n‚úÖ Merge complete: {len(dashboard_df)} rows preserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb313fa",
   "metadata": {},
   "source": [
    "## 4. Validate Final Schema and Data Quality\n",
    "\n",
    "Ensure the merged dataset matches the exact required schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e0cfee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Final Schema Validation:\n",
      "   Columns: ['district_id', 'month', 'migration_score', 'migration_category', 'peri_urban_label', 'digital_exclusion_score', 'risk_level']\n",
      "   Expected: ['district_id', 'month', 'migration_score', 'migration_category', 'peri_urban_label', 'digital_exclusion_score', 'risk_level']\n",
      "   ‚úÖ Schema matches exactly\n",
      "\n",
      "‚úÖ No missing values\n",
      "\n",
      "üìä Data Types:\n",
      "   district_id: object\n",
      "   month: object\n",
      "   migration_score: float64\n",
      "   migration_category: object\n",
      "   peri_urban_label: object\n",
      "   digital_exclusion_score: int64\n",
      "   risk_level: object\n",
      "\n",
      "üìÑ Sample merged data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district_id</th>\n",
       "      <th>month</th>\n",
       "      <th>migration_score</th>\n",
       "      <th>migration_category</th>\n",
       "      <th>peri_urban_label</th>\n",
       "      <th>digital_exclusion_score</th>\n",
       "      <th>risk_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-01</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>Moderate Inflow</td>\n",
       "      <td>Stable Urban</td>\n",
       "      <td>25</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-02</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>High Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>28</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>Moderate Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>31</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-04</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>Moderate Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>25</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-05</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>High Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>24</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-06</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>High Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>25</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-07</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>Moderate Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>31</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-08</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>Moderate Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>24</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-09</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>High Inflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>30</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GJ_AMD</td>\n",
       "      <td>2024-10</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>Moderate Outflow</td>\n",
       "      <td>Declining</td>\n",
       "      <td>25</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  district_id    month  migration_score migration_category peri_urban_label  \\\n",
       "0      GJ_AMD  2024-01           0.0017    Moderate Inflow     Stable Urban   \n",
       "1      GJ_AMD  2024-02           0.0154        High Inflow        Declining   \n",
       "2      GJ_AMD  2024-03           0.0105    Moderate Inflow        Declining   \n",
       "3      GJ_AMD  2024-04           0.0006    Moderate Inflow        Declining   \n",
       "4      GJ_AMD  2024-05           0.0246        High Inflow        Declining   \n",
       "5      GJ_AMD  2024-06           0.0202        High Inflow        Declining   \n",
       "6      GJ_AMD  2024-07           0.0054    Moderate Inflow        Declining   \n",
       "7      GJ_AMD  2024-08           0.0144    Moderate Inflow        Declining   \n",
       "8      GJ_AMD  2024-09           0.0211        High Inflow        Declining   \n",
       "9      GJ_AMD  2024-10          -0.0023   Moderate Outflow        Declining   \n",
       "\n",
       "   digital_exclusion_score risk_level  \n",
       "0                       25        Low  \n",
       "1                       28        Low  \n",
       "2                       31        Low  \n",
       "3                       25        Low  \n",
       "4                       24        Low  \n",
       "5                       25        Low  \n",
       "6                       31        Low  \n",
       "7                       24        Low  \n",
       "8                       30        Low  \n",
       "9                       25        Low  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validate Final Schema and Data Quality\n",
    "--------------------------------------\n",
    "Check that the merged dataset conforms to the exact API schema.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# FINAL SCHEMA DEFINITION (SOURCE OF TRUTH)\n",
    "# =============================================================================\n",
    "FINAL_COLUMNS = [\n",
    "    'district_id',           # string - stable district identifier\n",
    "    'month',                 # string - YYYY-MM format\n",
    "    'migration_score',       # float - normalized migration intensity\n",
    "    'migration_category',    # string - categorical label\n",
    "    'peri_urban_label',      # string - urbanization classification\n",
    "    'digital_exclusion_score',  # integer - 0-100 risk index\n",
    "    'risk_level'             # string - Low/Medium/High\n",
    "]\n",
    "\n",
    "# Reorder columns to match final schema\n",
    "dashboard_df = dashboard_df[FINAL_COLUMNS]\n",
    "\n",
    "# Sort by district_id and month for consistent output\n",
    "dashboard_df = dashboard_df.sort_values(['district_id', 'month']).reset_index(drop=True)\n",
    "\n",
    "print(\"üìã Final Schema Validation:\")\n",
    "print(f\"   Columns: {list(dashboard_df.columns)}\")\n",
    "print(f\"   Expected: {FINAL_COLUMNS}\")\n",
    "assert list(dashboard_df.columns) == FINAL_COLUMNS, \"‚ùå Schema mismatch!\"\n",
    "print(\"   ‚úÖ Schema matches exactly\")\n",
    "\n",
    "# Check for missing values\n",
    "null_counts = dashboard_df.isnull().sum()\n",
    "if null_counts.sum() > 0:\n",
    "    print(f\"\\n‚ùå Missing values found:\")\n",
    "    print(null_counts[null_counts > 0])\n",
    "else:\n",
    "    print(f\"\\n‚úÖ No missing values\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nüìä Data Types:\")\n",
    "for col in FINAL_COLUMNS:\n",
    "    print(f\"   {col}: {dashboard_df[col].dtype}\")\n",
    "\n",
    "# Preview merged data\n",
    "print(f\"\\nüìÑ Sample merged data:\")\n",
    "dashboard_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf20bcb",
   "metadata": {},
   "source": [
    "## 5. Export CSV (snake_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5c48539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV exported to: d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\api_ready\\aadhaar_pulse_dashboard.csv\n",
      "   Rows: 240\n",
      "   Columns: ['district_id', 'month', 'migration_score', 'migration_category', 'peri_urban_label', 'digital_exclusion_score', 'risk_level']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export CSV\n",
    "----------\n",
    "Save the merged dataset in CSV format with snake_case column names.\n",
    "This file will be read by the backend API.\n",
    "\"\"\"\n",
    "\n",
    "# CSV already uses snake_case column names\n",
    "dashboard_df.to_csv(CSV_OUTPUT_PATH, index=False)\n",
    "\n",
    "print(f\"‚úÖ CSV exported to: {CSV_OUTPUT_PATH}\")\n",
    "print(f\"   Rows: {len(dashboard_df)}\")\n",
    "print(f\"   Columns: {list(dashboard_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3ace8",
   "metadata": {},
   "source": [
    "## 6. Export JSON (camelCase)\n",
    "\n",
    "Convert column names to camelCase for JSON output (frontend convention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36cd7476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JSON exported to: d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\api_ready\\aadhaar_pulse_dashboard.json\n",
      "   Records: 240\n",
      "   Keys: ['districtId', 'month', 'migrationScore', 'migrationCategory', 'periUrbanLabel', 'digitalExclusionScore', 'riskLevel']\n",
      "\n",
      "üìÑ Sample JSON record:\n",
      "{\n",
      "  \"districtId\": \"GJ_AMD\",\n",
      "  \"month\": \"2024-01\",\n",
      "  \"migrationScore\": 0.0017,\n",
      "  \"migrationCategory\": \"Moderate Inflow\",\n",
      "  \"periUrbanLabel\": \"Stable Urban\",\n",
      "  \"digitalExclusionScore\": 25,\n",
      "  \"riskLevel\": \"Low\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Export JSON (camelCase)\n",
    "-----------------------\n",
    "Convert snake_case to camelCase for frontend consumption.\n",
    "Output is an array of objects.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# CAMELCASE COLUMN MAPPING\n",
    "# =============================================================================\n",
    "CAMEL_CASE_MAPPING = {\n",
    "    'district_id': 'districtId',\n",
    "    'month': 'month',  # No change needed\n",
    "    'migration_score': 'migrationScore',\n",
    "    'migration_category': 'migrationCategory',\n",
    "    'peri_urban_label': 'periUrbanLabel',\n",
    "    'digital_exclusion_score': 'digitalExclusionScore',\n",
    "    'risk_level': 'riskLevel'\n",
    "}\n",
    "\n",
    "# Create JSON-ready dataframe with camelCase columns\n",
    "json_df = dashboard_df.rename(columns=CAMEL_CASE_MAPPING)\n",
    "\n",
    "# Convert to list of dictionaries (array of objects)\n",
    "json_data = json_df.to_dict(orient='records')\n",
    "\n",
    "# Export to JSON file\n",
    "with open(JSON_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ JSON exported to: {JSON_OUTPUT_PATH}\")\n",
    "print(f\"   Records: {len(json_data)}\")\n",
    "print(f\"   Keys: {list(json_data[0].keys()) if json_data else 'N/A'}\")\n",
    "print(f\"\\nüìÑ Sample JSON record:\")\n",
    "print(json.dumps(json_data[0], indent=2) if json_data else 'N/A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15881b89",
   "metadata": {},
   "source": [
    "## 7. Final Summary\n",
    "\n",
    "Phase 6 complete. Dashboard data is now backend-ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1b7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 6 COMPLETE: Dashboard Data Merge\n",
      "============================================================\n",
      "\n",
      "üìÅ Output Files Created:\n",
      "   1. d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\api_ready\\aadhaar_pulse_dashboard.csv\n",
      "   2. d:\\Projects\\ML - DataScience\\Saarthi-Net-Data-Pipeline\\data\\api_ready\\aadhaar_pulse_dashboard.json\n",
      "\n",
      "üìä Dataset Statistics:\n",
      "   Total records: 240\n",
      "   Districts: 20\n",
      "   Months: 12\n",
      "\n",
      "üìã Final Schema (CSV - snake_case):\n",
      "   ‚Ä¢ district_id\n",
      "   ‚Ä¢ month\n",
      "   ‚Ä¢ migration_score\n",
      "   ‚Ä¢ migration_category\n",
      "   ‚Ä¢ peri_urban_label\n",
      "   ‚Ä¢ digital_exclusion_score\n",
      "   ‚Ä¢ risk_level\n",
      "\n",
      "üìã Final Schema (JSON - camelCase):\n",
      "   ‚Ä¢ districtId\n",
      "   ‚Ä¢ month\n",
      "   ‚Ä¢ migrationScore\n",
      "   ‚Ä¢ migrationCategory\n",
      "   ‚Ä¢ periUrbanLabel\n",
      "   ‚Ä¢ digitalExclusionScore\n",
      "   ‚Ä¢ riskLevel\n",
      "\n",
      "‚úÖ Data is ready for backend API and frontend dashboard!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Final Summary\n",
    "-------------\n",
    "Display summary statistics for the merged dashboard dataset.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 6 COMPLETE: Dashboard Data Merge\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìÅ Output Files Created:\")\n",
    "print(f\"   1. {CSV_OUTPUT_PATH}\")\n",
    "print(f\"   2. {JSON_OUTPUT_PATH}\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   Total records: {len(dashboard_df)}\")\n",
    "print(f\"   Districts: {dashboard_df['district_id'].nunique()}\")\n",
    "print(f\"   Months: {dashboard_df['month'].nunique()}\")\n",
    "\n",
    "print(f\"\\nüìã Final Schema (CSV - snake_case):\")\n",
    "for col in FINAL_COLUMNS:\n",
    "    print(f\"   ‚Ä¢ {col}\")\n",
    "\n",
    "print(f\"\\nüìã Final Schema (JSON - camelCase):\")\n",
    "for snake, camel in CAMEL_CASE_MAPPING.items():\n",
    "    print(f\"   ‚Ä¢ {camel}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data is ready for backend API and frontend dashboard!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
